{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not included in the end-stage of this exercise, this is just a tangential exploration.  #Sidequest mini-game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tl;dr:\n",
    "To computers, letters are just fancy-lookin' numbers.  Words can be used in numerical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using a pre-generated word-bank, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('EffectWordNet.tsv',delimiter='\\t',header=None)\n",
    "df.columns = ['some_number','polarity','word','definition']\n",
    "df['polarity'] = [1 if p == '+Effect' else -1 if p == '-Effect' else 0 for p in df['polarity']]\n",
    "df = df[['polarity','word']]\n",
    "\n",
    "pos = df[0:3298]\n",
    "neg = df[3299:5726]\n",
    "nul = df[5727:]\n",
    "\n",
    "pos_words = [','.join(pos['word'].values)]\n",
    "pos_df = pd.DataFrame([w.split(',') for w in pos_words]).transpose()\n",
    "pos_df.rename(columns={0:'word'},inplace=True)\n",
    "pos_df['polarity'] = 1\n",
    "\n",
    "neg_words = [','.join(neg['word'].values)]\n",
    "neg_df = pd.DataFrame([w.split(',') for w in neg_words]).transpose()\n",
    "neg_df.rename(columns={0:'word'},inplace=True)\n",
    "neg_df['polarity'] = -1\n",
    "\n",
    "def get_score(tokens):\n",
    "    score = 0\n",
    "\n",
    "    for t in tokens:\n",
    "        if t.lower() in pos_df['word'].values:\n",
    "            score += 1\n",
    "\n",
    "        if t.lower() in neg_df['word'].values:\n",
    "            score -= 1\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = 'http://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=game-of-thrones&episode='\n",
    "seasons = 6\n",
    "episodes = 10\n",
    "\n",
    "def all_urls(seasons, episodes):\n",
    "    all_urls = []\n",
    "    for s in range(1,seasons+1):\n",
    "        for e in range(1,episodes+1):\n",
    "            full_url = base+'s{}e{}'.format(format(s,'02d'),format(e,'02d'))\n",
    "            #print full_url\n",
    "            all_urls.append(full_url)\n",
    "    return all_urls\n",
    "\n",
    "def tokenize_got_script(txt_file):\n",
    "    contractions = {'\\'m':'am','\\'re':'are','\\'ve':'have','\\'d':'would','\\'ll':'will','n\\'t':'not'}\n",
    "    tokens = nltk.word_tokenize(txt_file)\n",
    "    tokens = [t.replace(t,contractions[t]) if t in contractions else t for t in tokens]\n",
    "    tokens = [t for t in tokens if t.isalpha() == True]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting token frequency - how often is each word said?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01e01 01e02 01e03 01e04 01e05 01e06 01e07 01e08 01e09 01e10 02e01 02e02 02e03 02e04 02e05 02e06 02e07 02e08 02e09 02e10 03e01 03e02 03e03 03e04 03e05 03e06 03e07 03e08 03e09 03e10 04e01 04e02 04e03 04e04 04e05 04e06 04e07 04e08 04e09 04e10 05e01 05e02 05e03 05e04 05e05 05e06 05e07 05e08 05e09 05e10 06e01 06e02 06e03 06e04 06e05 06e06 06e07 06e08 06e09 06e10\n"
     ]
    }
   ],
   "source": [
    "token_df = pd.DataFrame()\n",
    "\n",
    "for url in all_urls(seasons,episodes):\n",
    "    \n",
    "    episode = str(url[-5:])\n",
    "    filename = 'got_script_' + episode + '.txt'\n",
    "    print episode,\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "        tokens = tokenize_got_script(text.decode('utf-8'))\n",
    "        tmp = pd.DataFrame()\n",
    "        tmp['token'] = tokens\n",
    "        tmp['count'] = 1\n",
    "        tmp = tmp.groupby('token').count()\n",
    "        tmp['episode'] = filename\n",
    "        tmp.reset_index(inplace=True)\n",
    "        tmp.sort_values(by='count',ascending=False,inplace=True)\n",
    "        token_df = token_df.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_df['epID'] = [x[12] + x[-6:-4] for x in token_df['episode']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency of a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x117bfb990>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_for_word = 'father'\n",
    "\n",
    "token_df[token_df['token'] == check_for_word].sort_values(by='episode',ascending=True).plot(x='epID',\n",
    "                                                                                            y='count',\n",
    "                                                                                            title='How Many Times Was The Word \"{}\" Used in Game of Thrones?'.format(check_for_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11c432a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_for_word = 'nothing'\n",
    "\n",
    "token_df[token_df['token'] == check_for_word].sort_values(by='episode',ascending=True).plot(x='epID',\n",
    "                                                                                            y='count',\n",
    "                                                                                            title='How Many Times Was The Word \"{}\" Used in Game of Thrones?'.format(check_for_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Top 10 pos + neg words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_df = pd.read_csv('EffectWordNet.tsv',delimiter='\\t',header=None)\n",
    "s_df.columns = ['some_number','polarity','word','definition']\n",
    "s_df['polarity'] = [1 if p == '+Effect' else -1 if p == '-Effect' else 0 for p in s_df['polarity']]\n",
    "s_df = s_df[['polarity','word']]\n",
    "\n",
    "pos = df[df['polarity'] == 1]\n",
    "neg = df[df['polarity'] == -1]\n",
    "nul = df[df['polarity'] == 0]\n",
    "\n",
    "pos_words = ','.join(pos['word'].values)\n",
    "tmp_pos = pd.DataFrame([w.split(',') for w in [pos_words]]).transpose()\n",
    "tmp_pos.rename(columns={0:'word'},inplace=True)\n",
    "strip_from_pos = ['have','do','will','be','know','lord','can','come','go','back','get','let','tell','mean','fire','cut',\n",
    "                  'burn','fool','rule','reek','winter','fall','crow','bet','point','refuse','surrender','slave',\n",
    "                  'escape','obey','abandon','bottom','break','short','pass','strike','assume','judge','harbor',\n",
    "                  'pig','hunt','hurry','swing','crane','ice','lock','spike','argue','avoid','wench','gutter','dung',\n",
    "                  'reject','rough','toast','devise','devour','pervert','like','down','take','man','up','want',\n",
    "                  'make','master']\n",
    "pos_df = pd.DataFrame()\n",
    "pos_df['word'] = [w for w in tmp_pos['word'] if w not in strip_from_pos]\n",
    "pos_df['polarity'] = 1\n",
    "\n",
    "add_to_neg = ',imp,eunuch,kingslayer,wildling,beast,whore,fire,winter,hunt,pervert,bastard,cunt,fuck,shit,damn,damned,ugly'\n",
    "neg_words = ','.join(neg['word'].values)\n",
    "neg_words = neg_words + add_to_neg\n",
    "tmp_neg = pd.DataFrame([w.split(',') for w in [neg_words]]).transpose()\n",
    "tmp_neg.rename(columns={0:'word'},inplace=True)\n",
    "strip_from_neg = ['have','do','be','am','like','at','me','he','be','a','go','get','of','for','l','lt','be','me','he',\n",
    "                  'give','show','course','wonder','sun','ser','but','even','keep','free','put','return','found','wish',\n",
    "                  'out','over','bring','bit','work','move','visit','turn','set','fine','sing','support','touch','sight',\n",
    "                  'vow','grow','experience','book','fish','rise','continue','cook','center','joy','rescue','invite',\n",
    "                  'inspire','honeycomb','so','up','send','try','can','come','let','tell','make','better','hand',\n",
    "                  'place','hold','stay','call','best','live','stand','care','light','talk','face','lot','ride','feel',\n",
    "                  'save','water','start','full','eat','gate','happen','offer','carry','play','mouth','pray','belong',\n",
    "                  'silver','smart','cat','dog','share','fly','write''wash','counsel','milk','nose','stew','finish',\n",
    "                  'seat','swing','advise','calm','deliver','floor','attend','cheese','double','seek','cabin','accompany',\n",
    "                  'weather','badge','flag','say','off','ask','close','wash','require','jug','abstract','occur','devote',\n",
    "                  'stop','take','down','want']\n",
    "#retro-adjustment\n",
    "neg_df = pd.DataFrame()\n",
    "neg_df['word'] = [w for w in tmp_neg['word'] if w not in strip_from_neg]\n",
    "neg_df['polarity'] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check for any missing words in bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = 'ugly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check to see if a word is in the pos bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test in pos_df['word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check to see if a word is in the neg bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test in neg_df['word'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Bank QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2167a8ece8db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_TEST'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ctirol/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1992\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ctirol/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1997\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1999\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ctirol/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ctirol/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3225\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3226\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ctirol/anaconda2/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1876\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1878\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1880\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4027)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3891)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12408)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12359)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'token'"
     ]
    }
   ],
   "source": [
    "df['token_TEST'] = [x.lower() for x in df['token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['token_TEST','count','epID']][df['token_TEST'].isin(pos_df['word'])].sort_values(by='count',ascending=False).to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['token_TEST','count']][df['token_TEST'].isin(pos_df['word'])].sort_values(by='count',ascending=False).groupby('token_TEST').sum().reset_index().to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Bank QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['token_TEST'] = [x.lower() for x in df['token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['token_TEST','count','epID']][df['token_TEST'].isin(neg_df['word'])].sort_values(by='count',ascending=False).to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['token_TEST','count']][df['token_TEST'].isin(neg_df['word'])].sort_values(by='count',ascending=False).groupby('token_TEST').sum().reset_index().to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Clouds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "','.join(df['token_TEST'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from os import path\n",
    "\n",
    "#d = path.dirname(__file__)\n",
    "\n",
    "# Read the whole text.\n",
    "#text = open(path.join(d, 'constitution.txt')).read()\n",
    "text = ','.join(df['token_TEST'].values)\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take relative word frequencies into account, lower max_font_size\n",
    "wordcloud = WordCloud(max_font_size=400, relative_scaling=.5, width=1000).generate(text)\n",
    "#plt.figure()\n",
    "#plt.imshow(wordcloud)\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "# The pil way (if you don't have matplotlib)\n",
    "image = wordcloud.to_image()\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
